{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["'from transformers import VideoMAEConfig, VideoMAEForPreTraining, VideoMAEModel\\n\\n# Initializing a VideoMAE videomae-base style configuration\\nconfiguration = VideoMAEConfig()\\n# Randomly initializing a model from the configuration\\n# model = VideoMAEModel(configuration)\\n# # Accessing the model configuration\\n# configuration = model.config\\nmodel = VideoMAEForPreTraining(configuration)\\nmodel'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["'''from transformers import VideoMAEConfig, VideoMAEForPreTraining, VideoMAEModel\n","\n","# Initializing a VideoMAE videomae-base style configuration\n","configuration = VideoMAEConfig()\n","# Randomly initializing a model from the configuration\n","# model = VideoMAEModel(configuration)\n","# # Accessing the model configuration\n","# configuration = model.config\n","model = VideoMAEForPreTraining(configuration)\n","model'''"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import datetime\n","import numpy as np\n","import time\n","import torch\n","import torch.backends.cudnn as cudnn\n","import json\n","import os\n","from pathlib import Path\n","from timm.models import create_model\n","from optim_factory import create_optimizer\n","from datasets import build_pretraining_dataset\n","from engine_for_pretraining import train_one_epoch\n","from utils import NativeScalerWithGradNormCount as NativeScaler\n","import utils\n","import modeling_pretrain\n","\n","\n","def get_model(args):\n","    print(f\"Creating model: {args.model}\")\n","    model = create_model(\n","        args.model,\n","        pretrained=False,\n","        drop_path_rate=args.drop_path,\n","        drop_block_rate=None,\n","        decoder_depth=args.decoder_depth,\n","        use_checkpoint=args.use_checkpoint\n","    )\n","    return model\n","\n","'''\n","def main(args):\n","    utils.init_distributed_mode(args)\n","\n","    print(args)\n","\n","    device = torch.device(args.device)\n","\n","    # fix the seed for reproducibility\n","    seed = args.seed + utils.get_rank()\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","\n","    cudnn.benchmark = True\n","\n","    model = get_model(args)\n","    patch_size = model.encoder.patch_embed.patch_size\n","    print(\"Patch size = %s\" % str(patch_size))\n","    args.window_size = (args.num_frames // 2, args.input_size // patch_size[0], args.input_size // patch_size[1])\n","    args.patch_size = patch_size\n","\n","    # get dataset\n","    dataset_train = build_pretraining_dataset(args)\n","\n","\n","    num_tasks = utils.get_world_size()\n","    global_rank = utils.get_rank()\n","    sampler_rank = global_rank\n","\n","    total_batch_size = args.batch_size * num_tasks\n","    num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n","\n","    sampler_train = torch.utils.data.DistributedSampler(\n","        dataset_train, num_replicas=num_tasks, rank=sampler_rank, shuffle=True\n","    )\n","    print(\"Sampler_train = %s\" % str(sampler_train))\n","\n","\n","    if global_rank == 0 and args.log_dir is not None:\n","        os.makedirs(args.log_dir, exist_ok=True)\n","        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n","    else:\n","        log_writer = None\n","\n","    data_loader_train = torch.utils.data.DataLoader(\n","        dataset_train, sampler=sampler_train,\n","        batch_size=args.batch_size,\n","        num_workers=args.num_workers,\n","        pin_memory=args.pin_mem,\n","        drop_last=True,\n","        worker_init_fn=utils.seed_worker\n","    )\n","\n","    model.to(device)\n","    model_without_ddp = model\n","    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","    print(\"Model = %s\" % str(model_without_ddp))\n","    print('number of params: {} M'.format(n_parameters / 1e6))\n","\n","    args.lr = args.lr * total_batch_size / 256\n","    args.min_lr = args.min_lr * total_batch_size / 256\n","    args.warmup_lr = args.warmup_lr * total_batch_size / 256\n","    print(\"LR = %.8f\" % args.lr)\n","    print(\"Batch size = %d\" % total_batch_size)\n","    print(\"Number of training steps = %d\" % num_training_steps_per_epoch)\n","    print(\"Number of training examples per epoch = %d\" % (total_batch_size * num_training_steps_per_epoch))\n","\n","    if args.distributed:\n","        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=False)\n","        model_without_ddp = model.module\n","\n","    optimizer = create_optimizer(\n","        args, model_without_ddp)\n","    loss_scaler = NativeScaler()\n","\n","    print(\"Use step level LR & WD scheduler!\")\n","    lr_schedule_values = utils.cosine_scheduler(\n","        args.lr, args.min_lr, args.epochs, num_training_steps_per_epoch,\n","        warmup_epochs=args.warmup_epochs, warmup_steps=args.warmup_steps,\n","    )\n","    if args.weight_decay_end is None:\n","        args.weight_decay_end = args.weight_decay\n","    wd_schedule_values = utils.cosine_scheduler(\n","        args.weight_decay, args.weight_decay_end, args.epochs, num_training_steps_per_epoch)\n","    print(\"Max WD = %.7f, Min WD = %.7f\" % (max(wd_schedule_values), min(wd_schedule_values)))\n","\n","    utils.auto_load_model(\n","        args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)\n","    torch.cuda.empty_cache()\n","    print(f\"Start training for {args.epochs} epochs\")\n","    start_time = time.time()\n","    for epoch in range(args.start_epoch, args.epochs):\n","        if args.distributed:\n","            data_loader_train.sampler.set_epoch(epoch)\n","        if log_writer is not None:\n","            log_writer.set_step(epoch * num_training_steps_per_epoch)\n","        train_stats = train_one_epoch(\n","            model, data_loader_train,\n","            optimizer, device, epoch, loss_scaler,\n","            args.clip_grad, log_writer=log_writer,\n","            start_steps=epoch * num_training_steps_per_epoch,\n","            lr_schedule_values=lr_schedule_values,\n","            wd_schedule_values=wd_schedule_values,\n","            patch_size=patch_size[0],\n","            normlize_target=args.normlize_target,\n","        )\n","        if args.output_dir:\n","            if (epoch + 1) % args.save_ckpt_freq == 0 or epoch + 1 == args.epochs:\n","                utils.save_model(\n","                    args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n","                    loss_scaler=loss_scaler, epoch=epoch)\n","\n","        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n","                     'epoch': epoch, 'n_parameters': n_parameters}\n","\n","        if args.output_dir and utils.is_main_process():\n","            if log_writer is not None:\n","                log_writer.flush()\n","            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n","                f.write(json.dumps(log_stats) + \"\\n\")\n","\n","    total_time = time.time() - start_time\n","    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","    print('Training time {}'.format(total_time_str))\n","'''\n","from dataclasses import dataclass\n","\n","@dataclass\n","class Config:\n","    batch_size: int = 64\n","    epochs: int = 800\n","    save_ckpt_freq: int = 50\n","    model: str = 'pretrain_videomae_base_patch16_224'\n","    decoder_depth: int = 4\n","    mask_type: str = 'tube'\n","    mask_ratio: float = 0.75\n","    input_size: int = 224\n","    drop_path: float = 0.0\n","    normlize_target: bool = True\n","    opt: str = 'adamw'\n","    opt_eps: float = 1e-08\n","    opt_betas: tuple = None\n","    clip_grad: float = None\n","    momentum: float = 0.9\n","    weight_decay: float = 0.05\n","    weight_decay_end: float = None\n","    lr: float = 0.00015\n","    warmup_lr: float = 1e-06\n","    min_lr: float = 1e-05\n","    warmup_epochs: int = 40\n","    warmup_steps: int = -1\n","    use_checkpoint: bool = False\n","    color_jitter: float = 0.0\n","    train_interpolation: str = 'bicubic'\n","    data_path: str = '/kinetics-400'\n","    imagenet_default_mean_and_std: bool = True\n","    num_frames: int = 16\n","    sampling_rate: int = 4\n","    output_dir: str = 'experiment_artifact'\n","    log_dir: str = None\n","    device: str = 'cuda'\n","    seed: int = 0\n","    resume: str = ''\n","    auto_resume: bool = True\n","    start_epoch: int = 0\n","    num_workers: int = 10\n","    pin_mem: bool = True\n","    world_size: int = 1\n","    local_rank: int = -1\n","    dist_on_itp: bool = False\n","    dist_url: str = 'env://'\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# opts = get_args()\n","args = Config()\n","if args.output_dir:\n","    Path(args.output_dir).mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Not using distributed mode\n"]}],"source":["utils.init_distributed_mode(args)\n","device = torch.device(args.device)\n","# fix the seed for reproducibility\n","seed = args.seed + utils.get_rank()\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","cudnn.benchmark = True"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating model: pretrain_videomae_base_patch16_224\n","Patch size = (16, 16)\n"]}],"source":["model = get_model(args)\n","patch_size = model.encoder.patch_embed.patch_size\n","print(\"Patch size = %s\" % str(patch_size))\n","args.window_size = (args.num_frames // 2, args.input_size // patch_size[0], args.input_size // patch_size[1])\n","args.patch_size = patch_size"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["PretrainVisionTransformer(\n","  (encoder): PretrainVisionTransformerEncoder(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n","    )\n","    (blocks): ModuleList(\n","      (0): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (2): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (3): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (4): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (5): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (6): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (7): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (8): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (9): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (10): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (11): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    (head): Identity()\n","  )\n","  (decoder): PretrainVisionTransformerDecoder(\n","    (blocks): ModuleList(\n","      (0): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (2): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (3): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","    (head): Linear(in_features=384, out_features=1536, bias=True)\n","  )\n","  (encoder_to_decoder): Linear(in_features=768, out_features=384, bias=False)\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# torch.onnx.export(model, X_test, 'videoMAE.onnx', input_names=[\"features\"], output_names=[\"logits\"])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from masking_generator import TubeMaskingGenerator\n","tube_mask_generarator = TubeMaskingGenerator(args.window_size, args.mask_ratio)\n","from datasets import  DataAugmentationForVideoMAE\n","data_agumentor = DataAugmentationForVideoMAE(args)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Aug = (DataAugmentationForVideoMAE,\n","  transform = Compose(\n","    <transforms.GroupMultiScaleCrop object at 0x7f529f7a6410>\n","    <transforms.Stack object at 0x7f529f7a6500>\n","    <transforms.ToTorchFormatTensor object at 0x7f529f7a63b0>\n","    <transforms.GroupNormalize object at 0x7f529f7b6740>\n","),\n","  Masked position generator = Maks: total patches 1568, mask patches 1176,\n",")\n"]}],"source":["from kinetics import VideoMAE\n","def build_pretraining_dataset(args):\n","    transform = DataAugmentationForVideoMAE(args)\n","    dataset = VideoMAE(\n","        root=None,\n","        setting=args.data_path,\n","        video_ext='mp4',\n","        is_color=True,\n","        modality='rgb',\n","        new_length=args.num_frames,\n","        new_step=args.sampling_rate,\n","        transform=transform,\n","        temporal_jitter=False,\n","        video_loader=True,\n","        use_decord=True,\n","        lazy_init=True)\n","    print(\"Data Aug = %s\" % str(transform))\n","    return dataset\n","\n","dataset_train = build_pretraining_dataset(args)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["224"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import glob\n","from itertools import repeat\n","test_video_files = glob.glob(\"/home/jahid/Downloads/VideoMAE/kinetics-400/*.mp4\")\n","\n","for i in range(5):  #copy it n times\n","    test_video_files += test_video_files\n","\n","dataset_train.clips = list(zip(test_video_files, repeat(None, len(test_video_files))))\n","len(test_video_files)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 16, 224, 224])\n"]}],"source":["for d in dataset_train:\n","    print(d[0].shape)\n","    break"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f529f7c0280>\n"]}],"source":["num_tasks = utils.get_world_size()\n","global_rank = utils.get_rank()\n","sampler_rank = global_rank\n","\n","total_batch_size = args.batch_size * num_tasks\n","num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n","\n","sampler_train = torch.utils.data.DistributedSampler(\n","    dataset_train, num_replicas=num_tasks, rank=sampler_rank, shuffle=True\n",")\n","print(\"Sampler_train = %s\" % str(sampler_train))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["if global_rank == 0 and args.log_dir is not None:\n","    os.makedirs(args.log_dir, exist_ok=True)\n","    log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n","else:\n","    log_writer = None"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["data_loader_train = torch.utils.data.DataLoader(\n","    dataset_train, sampler=sampler_train,\n","    batch_size=2,#args.batch_size,\n","    num_workers=args.num_workers,\n","    pin_memory=args.pin_mem,\n","    drop_last=True,\n","    worker_init_fn=utils.seed_worker\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model = PretrainVisionTransformer(\n","  (encoder): PretrainVisionTransformerEncoder(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n","    )\n","    (blocks): ModuleList(\n","      (0): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (2): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (3): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (4): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (5): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (6): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (7): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (8): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (9): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (10): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (11): Block(\n","        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=768, out_features=768, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    (head): Identity()\n","  )\n","  (decoder): PretrainVisionTransformerDecoder(\n","    (blocks): ModuleList(\n","      (0): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (2): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (3): Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","    (head): Linear(in_features=384, out_features=1536, bias=True)\n","  )\n","  (encoder_to_decoder): Linear(in_features=768, out_features=384, bias=False)\n",")\n","number of params: 94.210944 M\n","LR = 0.00003750\n","Batch size = 64\n","Number of training steps = 3\n","Number of training examples per epoch = 192\n"]}],"source":["model.to(device)\n","model_without_ddp = model\n","n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Model = %s\" % str(model_without_ddp))\n","print('number of params: {} M'.format(n_parameters / 1e6))\n","\n","args.lr = args.lr * total_batch_size / 256\n","args.min_lr = args.min_lr * total_batch_size / 256\n","args.warmup_lr = args.warmup_lr * total_batch_size / 256\n","print(\"LR = %.8f\" % args.lr)\n","print(\"Batch size = %d\" % total_batch_size)\n","print(\"Number of training steps = %d\" % num_training_steps_per_epoch)\n","print(\"Number of training examples per epoch = %d\" % (total_batch_size * num_training_steps_per_epoch))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Param groups = {\n","  \"no_decay\": {\n","    \"weight_decay\": 0.0,\n","    \"params\": [\n","      \"mask_token\",\n","      \"encoder.patch_embed.proj.bias\",\n","      \"encoder.blocks.0.norm1.weight\",\n","      \"encoder.blocks.0.norm1.bias\",\n","      \"encoder.blocks.0.attn.q_bias\",\n","      \"encoder.blocks.0.attn.v_bias\",\n","      \"encoder.blocks.0.attn.proj.bias\",\n","      \"encoder.blocks.0.norm2.weight\",\n","      \"encoder.blocks.0.norm2.bias\",\n","      \"encoder.blocks.0.mlp.fc1.bias\",\n","      \"encoder.blocks.0.mlp.fc2.bias\",\n","      \"encoder.blocks.1.norm1.weight\",\n","      \"encoder.blocks.1.norm1.bias\",\n","      \"encoder.blocks.1.attn.q_bias\",\n","      \"encoder.blocks.1.attn.v_bias\",\n","      \"encoder.blocks.1.attn.proj.bias\",\n","      \"encoder.blocks.1.norm2.weight\",\n","      \"encoder.blocks.1.norm2.bias\",\n","      \"encoder.blocks.1.mlp.fc1.bias\",\n","      \"encoder.blocks.1.mlp.fc2.bias\",\n","      \"encoder.blocks.2.norm1.weight\",\n","      \"encoder.blocks.2.norm1.bias\",\n","      \"encoder.blocks.2.attn.q_bias\",\n","      \"encoder.blocks.2.attn.v_bias\",\n","      \"encoder.blocks.2.attn.proj.bias\",\n","      \"encoder.blocks.2.norm2.weight\",\n","      \"encoder.blocks.2.norm2.bias\",\n","      \"encoder.blocks.2.mlp.fc1.bias\",\n","      \"encoder.blocks.2.mlp.fc2.bias\",\n","      \"encoder.blocks.3.norm1.weight\",\n","      \"encoder.blocks.3.norm1.bias\",\n","      \"encoder.blocks.3.attn.q_bias\",\n","      \"encoder.blocks.3.attn.v_bias\",\n","      \"encoder.blocks.3.attn.proj.bias\",\n","      \"encoder.blocks.3.norm2.weight\",\n","      \"encoder.blocks.3.norm2.bias\",\n","      \"encoder.blocks.3.mlp.fc1.bias\",\n","      \"encoder.blocks.3.mlp.fc2.bias\",\n","      \"encoder.blocks.4.norm1.weight\",\n","      \"encoder.blocks.4.norm1.bias\",\n","      \"encoder.blocks.4.attn.q_bias\",\n","      \"encoder.blocks.4.attn.v_bias\",\n","      \"encoder.blocks.4.attn.proj.bias\",\n","      \"encoder.blocks.4.norm2.weight\",\n","      \"encoder.blocks.4.norm2.bias\",\n","      \"encoder.blocks.4.mlp.fc1.bias\",\n","      \"encoder.blocks.4.mlp.fc2.bias\",\n","      \"encoder.blocks.5.norm1.weight\",\n","      \"encoder.blocks.5.norm1.bias\",\n","      \"encoder.blocks.5.attn.q_bias\",\n","      \"encoder.blocks.5.attn.v_bias\",\n","      \"encoder.blocks.5.attn.proj.bias\",\n","      \"encoder.blocks.5.norm2.weight\",\n","      \"encoder.blocks.5.norm2.bias\",\n","      \"encoder.blocks.5.mlp.fc1.bias\",\n","      \"encoder.blocks.5.mlp.fc2.bias\",\n","      \"encoder.blocks.6.norm1.weight\",\n","      \"encoder.blocks.6.norm1.bias\",\n","      \"encoder.blocks.6.attn.q_bias\",\n","      \"encoder.blocks.6.attn.v_bias\",\n","      \"encoder.blocks.6.attn.proj.bias\",\n","      \"encoder.blocks.6.norm2.weight\",\n","      \"encoder.blocks.6.norm2.bias\",\n","      \"encoder.blocks.6.mlp.fc1.bias\",\n","      \"encoder.blocks.6.mlp.fc2.bias\",\n","      \"encoder.blocks.7.norm1.weight\",\n","      \"encoder.blocks.7.norm1.bias\",\n","      \"encoder.blocks.7.attn.q_bias\",\n","      \"encoder.blocks.7.attn.v_bias\",\n","      \"encoder.blocks.7.attn.proj.bias\",\n","      \"encoder.blocks.7.norm2.weight\",\n","      \"encoder.blocks.7.norm2.bias\",\n","      \"encoder.blocks.7.mlp.fc1.bias\",\n","      \"encoder.blocks.7.mlp.fc2.bias\",\n","      \"encoder.blocks.8.norm1.weight\",\n","      \"encoder.blocks.8.norm1.bias\",\n","      \"encoder.blocks.8.attn.q_bias\",\n","      \"encoder.blocks.8.attn.v_bias\",\n","      \"encoder.blocks.8.attn.proj.bias\",\n","      \"encoder.blocks.8.norm2.weight\",\n","      \"encoder.blocks.8.norm2.bias\",\n","      \"encoder.blocks.8.mlp.fc1.bias\",\n","      \"encoder.blocks.8.mlp.fc2.bias\",\n","      \"encoder.blocks.9.norm1.weight\",\n","      \"encoder.blocks.9.norm1.bias\",\n","      \"encoder.blocks.9.attn.q_bias\",\n","      \"encoder.blocks.9.attn.v_bias\",\n","      \"encoder.blocks.9.attn.proj.bias\",\n","      \"encoder.blocks.9.norm2.weight\",\n","      \"encoder.blocks.9.norm2.bias\",\n","      \"encoder.blocks.9.mlp.fc1.bias\",\n","      \"encoder.blocks.9.mlp.fc2.bias\",\n","      \"encoder.blocks.10.norm1.weight\",\n","      \"encoder.blocks.10.norm1.bias\",\n","      \"encoder.blocks.10.attn.q_bias\",\n","      \"encoder.blocks.10.attn.v_bias\",\n","      \"encoder.blocks.10.attn.proj.bias\",\n","      \"encoder.blocks.10.norm2.weight\",\n","      \"encoder.blocks.10.norm2.bias\",\n","      \"encoder.blocks.10.mlp.fc1.bias\",\n","      \"encoder.blocks.10.mlp.fc2.bias\",\n","      \"encoder.blocks.11.norm1.weight\",\n","      \"encoder.blocks.11.norm1.bias\",\n","      \"encoder.blocks.11.attn.q_bias\",\n","      \"encoder.blocks.11.attn.v_bias\",\n","      \"encoder.blocks.11.attn.proj.bias\",\n","      \"encoder.blocks.11.norm2.weight\",\n","      \"encoder.blocks.11.norm2.bias\",\n","      \"encoder.blocks.11.mlp.fc1.bias\",\n","      \"encoder.blocks.11.mlp.fc2.bias\",\n","      \"encoder.norm.weight\",\n","      \"encoder.norm.bias\",\n","      \"decoder.blocks.0.norm1.weight\",\n","      \"decoder.blocks.0.norm1.bias\",\n","      \"decoder.blocks.0.attn.q_bias\",\n","      \"decoder.blocks.0.attn.v_bias\",\n","      \"decoder.blocks.0.attn.proj.bias\",\n","      \"decoder.blocks.0.norm2.weight\",\n","      \"decoder.blocks.0.norm2.bias\",\n","      \"decoder.blocks.0.mlp.fc1.bias\",\n","      \"decoder.blocks.0.mlp.fc2.bias\",\n","      \"decoder.blocks.1.norm1.weight\",\n","      \"decoder.blocks.1.norm1.bias\",\n","      \"decoder.blocks.1.attn.q_bias\",\n","      \"decoder.blocks.1.attn.v_bias\",\n","      \"decoder.blocks.1.attn.proj.bias\",\n","      \"decoder.blocks.1.norm2.weight\",\n","      \"decoder.blocks.1.norm2.bias\",\n","      \"decoder.blocks.1.mlp.fc1.bias\",\n","      \"decoder.blocks.1.mlp.fc2.bias\",\n","      \"decoder.blocks.2.norm1.weight\",\n","      \"decoder.blocks.2.norm1.bias\",\n","      \"decoder.blocks.2.attn.q_bias\",\n","      \"decoder.blocks.2.attn.v_bias\",\n","      \"decoder.blocks.2.attn.proj.bias\",\n","      \"decoder.blocks.2.norm2.weight\",\n","      \"decoder.blocks.2.norm2.bias\",\n","      \"decoder.blocks.2.mlp.fc1.bias\",\n","      \"decoder.blocks.2.mlp.fc2.bias\",\n","      \"decoder.blocks.3.norm1.weight\",\n","      \"decoder.blocks.3.norm1.bias\",\n","      \"decoder.blocks.3.attn.q_bias\",\n","      \"decoder.blocks.3.attn.v_bias\",\n","      \"decoder.blocks.3.attn.proj.bias\",\n","      \"decoder.blocks.3.norm2.weight\",\n","      \"decoder.blocks.3.norm2.bias\",\n","      \"decoder.blocks.3.mlp.fc1.bias\",\n","      \"decoder.blocks.3.mlp.fc2.bias\",\n","      \"decoder.norm.weight\",\n","      \"decoder.norm.bias\",\n","      \"decoder.head.bias\"\n","    ],\n","    \"lr_scale\": 1.0\n","  },\n","  \"decay\": {\n","    \"weight_decay\": 0.05,\n","    \"params\": [\n","      \"encoder.patch_embed.proj.weight\",\n","      \"encoder.blocks.0.attn.qkv.weight\",\n","      \"encoder.blocks.0.attn.proj.weight\",\n","      \"encoder.blocks.0.mlp.fc1.weight\",\n","      \"encoder.blocks.0.mlp.fc2.weight\",\n","      \"encoder.blocks.1.attn.qkv.weight\",\n","      \"encoder.blocks.1.attn.proj.weight\",\n","      \"encoder.blocks.1.mlp.fc1.weight\",\n","      \"encoder.blocks.1.mlp.fc2.weight\",\n","      \"encoder.blocks.2.attn.qkv.weight\",\n","      \"encoder.blocks.2.attn.proj.weight\",\n","      \"encoder.blocks.2.mlp.fc1.weight\",\n","      \"encoder.blocks.2.mlp.fc2.weight\",\n","      \"encoder.blocks.3.attn.qkv.weight\",\n","      \"encoder.blocks.3.attn.proj.weight\",\n","      \"encoder.blocks.3.mlp.fc1.weight\",\n","      \"encoder.blocks.3.mlp.fc2.weight\",\n","      \"encoder.blocks.4.attn.qkv.weight\",\n","      \"encoder.blocks.4.attn.proj.weight\",\n","      \"encoder.blocks.4.mlp.fc1.weight\",\n","      \"encoder.blocks.4.mlp.fc2.weight\",\n","      \"encoder.blocks.5.attn.qkv.weight\",\n","      \"encoder.blocks.5.attn.proj.weight\",\n","      \"encoder.blocks.5.mlp.fc1.weight\",\n","      \"encoder.blocks.5.mlp.fc2.weight\",\n","      \"encoder.blocks.6.attn.qkv.weight\",\n","      \"encoder.blocks.6.attn.proj.weight\",\n","      \"encoder.blocks.6.mlp.fc1.weight\",\n","      \"encoder.blocks.6.mlp.fc2.weight\",\n","      \"encoder.blocks.7.attn.qkv.weight\",\n","      \"encoder.blocks.7.attn.proj.weight\",\n","      \"encoder.blocks.7.mlp.fc1.weight\",\n","      \"encoder.blocks.7.mlp.fc2.weight\",\n","      \"encoder.blocks.8.attn.qkv.weight\",\n","      \"encoder.blocks.8.attn.proj.weight\",\n","      \"encoder.blocks.8.mlp.fc1.weight\",\n","      \"encoder.blocks.8.mlp.fc2.weight\",\n","      \"encoder.blocks.9.attn.qkv.weight\",\n","      \"encoder.blocks.9.attn.proj.weight\",\n","      \"encoder.blocks.9.mlp.fc1.weight\",\n","      \"encoder.blocks.9.mlp.fc2.weight\",\n","      \"encoder.blocks.10.attn.qkv.weight\",\n","      \"encoder.blocks.10.attn.proj.weight\",\n","      \"encoder.blocks.10.mlp.fc1.weight\",\n","      \"encoder.blocks.10.mlp.fc2.weight\",\n","      \"encoder.blocks.11.attn.qkv.weight\",\n","      \"encoder.blocks.11.attn.proj.weight\",\n","      \"encoder.blocks.11.mlp.fc1.weight\",\n","      \"encoder.blocks.11.mlp.fc2.weight\",\n","      \"decoder.blocks.0.attn.qkv.weight\",\n","      \"decoder.blocks.0.attn.proj.weight\",\n","      \"decoder.blocks.0.mlp.fc1.weight\",\n","      \"decoder.blocks.0.mlp.fc2.weight\",\n","      \"decoder.blocks.1.attn.qkv.weight\",\n","      \"decoder.blocks.1.attn.proj.weight\",\n","      \"decoder.blocks.1.mlp.fc1.weight\",\n","      \"decoder.blocks.1.mlp.fc2.weight\",\n","      \"decoder.blocks.2.attn.qkv.weight\",\n","      \"decoder.blocks.2.attn.proj.weight\",\n","      \"decoder.blocks.2.mlp.fc1.weight\",\n","      \"decoder.blocks.2.mlp.fc2.weight\",\n","      \"decoder.blocks.3.attn.qkv.weight\",\n","      \"decoder.blocks.3.attn.proj.weight\",\n","      \"decoder.blocks.3.mlp.fc1.weight\",\n","      \"decoder.blocks.3.mlp.fc2.weight\",\n","      \"decoder.head.weight\",\n","      \"encoder_to_decoder.weight\"\n","    ],\n","    \"lr_scale\": 1.0\n","  }\n","}\n","optimizer settings: {'lr': 3.75e-05, 'weight_decay': 0.0, 'eps': 1e-08}\n","Use step level LR & WD scheduler!\n","Set warmup steps = 120\n","Set warmup steps = 0\n","Max WD = 0.0500000, Min WD = 0.0500000\n","Auto resume checkpoint: \n"]}],"source":["if args.distributed:\n","    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=False)\n","    model_without_ddp = model.module\n","\n","optimizer = create_optimizer(\n","    args, model_without_ddp)\n","loss_scaler = NativeScaler()\n","\n","print(\"Use step level LR & WD scheduler!\")\n","lr_schedule_values = utils.cosine_scheduler(\n","    args.lr, args.min_lr, args.epochs, num_training_steps_per_epoch,\n","    warmup_epochs=args.warmup_epochs, warmup_steps=args.warmup_steps,\n",")\n","if args.weight_decay_end is None:\n","    args.weight_decay_end = args.weight_decay\n","wd_schedule_values = utils.cosine_scheduler(\n","    args.weight_decay, args.weight_decay_end, args.epochs, num_training_steps_per_epoch)\n","print(\"Max WD = %.7f, Min WD = %.7f\" % (max(wd_schedule_values), min(wd_schedule_values)))\n","\n","utils.auto_load_model(\n","    args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training for 800 epochs\n","Epoch: [0]  [  0/112]  eta: 0:06:08  lr: 0.000000  min_lr: 0.000000  loss: 1.4261 (1.4261)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9484 (0.9484)  time: 3.2900  data: 0.8717  max mem: 2248\n","Epoch: [0]  [ 10/112]  eta: 0:00:43  lr: 0.000003  min_lr: 0.000003  loss: 1.4139 (1.4000)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9269 (0.9199)  time: 0.4300  data: 0.0794  max mem: 3355\n","Epoch: [0]  [ 20/112]  eta: 0:00:26  lr: 0.000006  min_lr: 0.000006  loss: 1.3688 (1.3726)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8451 (0.8287)  time: 0.1428  data: 0.0002  max mem: 3355\n","Epoch: [0]  [ 30/112]  eta: 0:00:20  lr: 0.000009  min_lr: 0.000009  loss: 1.3056 (1.3451)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6295 (0.7423)  time: 0.1422  data: 0.0002  max mem: 3355\n","Epoch: [0]  [ 40/112]  eta: 0:00:15  lr: 0.000013  min_lr: 0.000013  loss: 1.2493 (1.3117)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.4891 (0.6716)  time: 0.1424  data: 0.0002  max mem: 3355\n","Epoch: [0]  [ 50/112]  eta: 0:00:12  lr: 0.000016  min_lr: 0.000016  loss: 1.1862 (1.2789)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.4078 (0.6135)  time: 0.1419  data: 0.0002  max mem: 3355\n","Epoch: [0]  [ 60/112]  eta: 0:00:10  lr: 0.000019  min_lr: 0.000019  loss: 1.1373 (1.2545)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.3418 (0.5648)  time: 0.1421  data: 0.0001  max mem: 3355\n","Epoch: [0]  [ 70/112]  eta: 0:00:07  lr: 0.000022  min_lr: 0.000022  loss: 1.1087 (1.2319)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.2912 (0.5232)  time: 0.1427  data: 0.0002  max mem: 3355\n","Epoch: [0]  [ 80/112]  eta: 0:00:05  lr: 0.000025  min_lr: 0.000025  loss: 1.0771 (1.2098)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.2395 (0.4856)  time: 0.1429  data: 0.0002  max mem: 3355\n","Epoch: [0]  [ 90/112]  eta: 0:00:03  lr: 0.000028  min_lr: 0.000028  loss: 1.0489 (1.1901)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.1956 (0.4516)  time: 0.1424  data: 0.0001  max mem: 3355\n","Epoch: [0]  [100/112]  eta: 0:00:02  lr: 0.000032  min_lr: 0.000032  loss: 1.0315 (1.1739)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.1576 (0.4206)  time: 0.1419  data: 0.0001  max mem: 3355\n","Epoch: [0]  [110/112]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000035  loss: 1.0205 (1.1597)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.1215 (0.3926)  time: 0.1419  data: 0.0001  max mem: 3355\n","Epoch: [0]  [111/112]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000035  loss: 1.0193 (1.1584)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.1184 (0.3899)  time: 0.1420  data: 0.0001  max mem: 3355\n","Epoch: [0] Total time: 0:00:19 (0.1712 s / it)\n","Averaged stats: lr: 0.000035  min_lr: 0.000035  loss: 1.0193 (1.1584)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.1184 (0.3899)\n","Epoch: [1]  [  0/112]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000001  loss: 1.0129 (1.0129)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.0858 (0.0858)  time: 1.0726  data: 0.8377  max mem: 3355\n","Epoch: [1]  [ 10/112]  eta: 0:00:23  lr: 0.000004  min_lr: 0.000004  loss: 1.0125 (0.9871)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.0877 (0.0874)  time: 0.2311  data: 0.0763  max mem: 3355\n","Epoch: [1]  [ 20/112]  eta: 0:00:17  lr: 0.000007  min_lr: 0.000007  loss: 1.0121 (0.9878)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.0876 (0.0878)  time: 0.1449  data: 0.0002  max mem: 3355\n","Epoch: [1]  [ 30/112]  eta: 0:00:14  lr: 0.000010  min_lr: 0.000010  loss: 1.0115 (0.9919)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.0853 (0.0868)  time: 0.1425  data: 0.0002  max mem: 3355\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     log_writer\u001b[38;5;241m.\u001b[39mset_step(epoch \u001b[38;5;241m*\u001b[39m num_training_steps_per_epoch)\n\u001b[0;32m----> 8\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_training_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_schedule_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_schedule_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwd_schedule_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd_schedule_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormlize_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormlize_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39msave_ckpt_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m args\u001b[38;5;241m.\u001b[39mepochs:\n","File \u001b[0;32m~/Downloads/VideoMAE/engine_for_pretraining.py:68\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device, epoch, loss_scaler, max_norm, patch_size, normlize_target, log_writer, lr_scheduler, start_steps, lr_schedule_values, wd_schedule_values)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# this attribute is added by timm on one optimizer (adahessian)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m is_second_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(optimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_second_order\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mis_second_order\n\u001b[0;32m---> 68\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mloss_scaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_second_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m loss_scale_value \u001b[38;5;241m=\u001b[39m loss_scaler\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     72\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n","File \u001b[0;32m~/Downloads/VideoMAE/utils.py:348\u001b[0m, in \u001b[0;36mNativeScalerWithGradNormCount.__call__\u001b[0;34m(self, loss, optimizer, clip_grad, parameters, create_graph, update_grad)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, optimizer, clip_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, update_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_grad:\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m clip_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/pt1.13/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/pt1.13/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print(f\"Start training for {args.epochs} epochs\")\n","start_time = time.time()\n","for epoch in range(args.start_epoch, args.epochs):\n","    if args.distributed:\n","        data_loader_train.sampler.set_epoch(epoch)\n","    if log_writer is not None:\n","        log_writer.set_step(epoch * num_training_steps_per_epoch)\n","    train_stats = train_one_epoch(\n","        model, data_loader_train,\n","        optimizer, device, epoch, loss_scaler,\n","        args.clip_grad, log_writer=log_writer,\n","        start_steps=epoch * num_training_steps_per_epoch,\n","        lr_schedule_values=lr_schedule_values,\n","        wd_schedule_values=wd_schedule_values,\n","        patch_size=patch_size[0],\n","        normlize_target=args.normlize_target,\n","    )\n","    if args.output_dir:\n","        if (epoch + 1) % args.save_ckpt_freq == 0 or epoch + 1 == args.epochs:\n","            utils.save_model(\n","                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n","                loss_scaler=loss_scaler, epoch=epoch)\n","\n","    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n","                    'epoch': epoch, 'n_parameters': n_parameters}\n","\n","    if args.output_dir and utils.is_main_process():\n","        if log_writer is not None:\n","            log_writer.flush()\n","        with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n","            f.write(json.dumps(log_stats) + \"\\n\")\n","\n","total_time = time.time() - start_time\n","total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","print('Training time {}'.format(total_time_str))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(256,) (16, 256) (4096,)\n"]}],"source":["import numpy as np\n","\n","frames = 16 # number of frame per video\n","num_patches_per_frame = 16*16\n","num_masks_per_frame = int(16*16*.9) # 90% masking as mentined in the paper \n","\n","mask_per_frame = np.hstack([\n","    np.zeros(num_patches_per_frame - num_masks_per_frame),  # non-masked patch\n","    np.ones(num_masks_per_frame), # masked patch\n","])\n","\n","np.random.shuffle(mask_per_frame)\n","mask_matrix = np.tile(mask_per_frame, (frames,1))\n","mask = mask_matrix.flatten()\n","\n","print(mask_per_frame.shape, mask_matrix.shape, mask.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on function tile in module numpy:\n","\n","tile(A, reps)\n","    Construct an array by repeating A the number of times given by reps.\n","    \n","    If `reps` has length ``d``, the result will have dimension of\n","    ``max(d, A.ndim)``.\n","    \n","    If ``A.ndim < d``, `A` is promoted to be d-dimensional by prepending new\n","    axes. So a shape (3,) array is promoted to (1, 3) for 2-D replication,\n","    or shape (1, 1, 3) for 3-D replication. If this is not the desired\n","    behavior, promote `A` to d-dimensions manually before calling this\n","    function.\n","    \n","    If ``A.ndim > d``, `reps` is promoted to `A`.ndim by pre-pending 1's to it.\n","    Thus for an `A` of shape (2, 3, 4, 5), a `reps` of (2, 2) is treated as\n","    (1, 1, 2, 2).\n","    \n","    Note : Although tile may be used for broadcasting, it is strongly\n","    recommended to use numpy's broadcasting operations and functions.\n","    \n","    Parameters\n","    ----------\n","    A : array_like\n","        The input array.\n","    reps : array_like\n","        The number of repetitions of `A` along each axis.\n","    \n","    Returns\n","    -------\n","    c : ndarray\n","        The tiled output array.\n","    \n","    See Also\n","    --------\n","    repeat : Repeat elements of an array.\n","    broadcast_to : Broadcast an array to a new shape\n","    \n","    Examples\n","    --------\n","    >>> a = np.array([0, 1, 2])\n","    >>> np.tile(a, 2)\n","    array([0, 1, 2, 0, 1, 2])\n","    >>> np.tile(a, (2, 2))\n","    array([[0, 1, 2, 0, 1, 2],\n","           [0, 1, 2, 0, 1, 2]])\n","    >>> np.tile(a, (2, 1, 2))\n","    array([[[0, 1, 2, 0, 1, 2]],\n","           [[0, 1, 2, 0, 1, 2]]])\n","    \n","    >>> b = np.array([[1, 2], [3, 4]])\n","    >>> np.tile(b, 2)\n","    array([[1, 2, 1, 2],\n","           [3, 4, 3, 4]])\n","    >>> np.tile(b, (2, 1))\n","    array([[1, 2],\n","           [3, 4],\n","           [1, 2],\n","           [3, 4]])\n","    \n","    >>> c = np.array([1,2,3,4])\n","    >>> np.tile(c,(4,1))\n","    array([[1, 2, 3, 4],\n","           [1, 2, 3, 4],\n","           [1, 2, 3, 4],\n","           [1, 2, 3, 4]])\n","\n"]}],"source":["help(np.tile)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(16, 256)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["mask_matrix.shape"]}],"metadata":{"kernelspec":{"display_name":"xtts","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
